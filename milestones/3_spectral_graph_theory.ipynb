{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'18] milestone 3: spectral graph theory\n",
    "[ntds'18]: https://github.com/mdeff/ntds_2018\n",
    "\n",
    "[Michaël Defferrard](http://deff.ch), [EPFL LTS2](https://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: 31\n",
    "* Students: Dilara Günay, Derin Sinan Bursa, Othman Benchekroun, Sinan Gökçe\n",
    "* Dataset: IMDb Films and Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to two sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this milestone is to get familiar with the graph Laplacian and its spectral decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Load your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a `No module named 'sklearn'` error when running the below cell, install [scikit-learn](https://scikit-learn.org) with `conda install scikit-learn` (after activating the `ntds_2018` environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote your graph as $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, A)$, where $\\mathcal{V}$ is the set of nodes, $\\mathcal{E}$ is the set of edges, $A \\in \\mathbb{R}^{N \\times N}$ is the (weighted) adjacency matrix, and $N = |\\mathcal{V}|$ is the number of nodes.\n",
    "\n",
    "Import the adjacency matrix $A$ that you constructed in the first milestone.\n",
    "(You're allowed to update it between milestones if you want to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting adjacency matrix\n",
    "import pandas as pd\n",
    "adjacency = pd.read_csv('data/adjacency.csv')\n",
    "n_nodes = len(adjacency)\n",
    "#Dropping useless column from adjacency dataframe\n",
    "adjacency.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "adjacency = adjacency.values\n",
    "np.set_printoptions(suppress = True)\n",
    "adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "From the (weighted) adjacency matrix $A$, compute both the combinatorial (also called unnormalized) and the normalized graph Laplacian matrices.\n",
    "\n",
    "Note: if your graph is weighted, use the weighted adjacency matrix. If not, use the binary adjacency matrix.\n",
    "\n",
    "For efficient storage and computation, store these sparse matrices in a [compressed sparse row (CSR) format](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros(shape=(n_nodes,n_nodes))\n",
    "for i in range(n_nodes):\n",
    "    sum = 0\n",
    "    for j in range(n_nodes):\n",
    "        sum = sum + adjacency[i,j]\n",
    "    D[i,i]=sum\n",
    "laplacian_combinatorial = D - adjacency #Some elements of spectral graph theory page 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(n_nodes,n_nodes)\n",
    "D_sqrt = scipy.linalg.fractional_matrix_power(D,-0.5)\n",
    "laplacian_normalized = I-np.matmul(np.matmul(D_sqrt,adjacency),D_sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of them as the graph Laplacian $L$ for the rest of the milestone.\n",
    "We however encourage you to run the code with both to get a sense of the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = laplacian_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compute the eigendecomposition of the Laplacian $L = U^\\top \\Lambda U$, where the columns $u_k \\in \\mathbb{R}^N$ of $U = [u_1, \\dots, u_N] \\in \\mathbb{R}^{N \\times N}$ are the eigenvectors and the diagonal elements $\\lambda_k = \\Lambda_{kk}$ are the corresponding eigenvalues.\n",
    "\n",
    "Make sure that the eigenvalues are ordered, i.e., $0 = \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues,eigenvectors = scipy.linalg.eigh(laplacian)\n",
    "assert eigenvectors.shape == (n_nodes, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justify your choice of eigensolver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Keeping in mind that our normalized graph Laplacian matrix (_laplacian__normalized_) is a real symmetrical matrix since the scipy.linalg.eigh is used to solve generalized eigenvalue problems, we have selected this eigensolver (_scipy.linalg.eigh_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "We can write $L = S S^\\top$. What is the matrix $S$? What does $S^\\top x$, with $x \\in \\mathbb{R}^N$, compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The matrix $S$ is the incidence matrix. <br>\n",
    "$x^\\top L x = x^\\top SS^\\top x = \\|S^\\top x\\|_2^2 $ <br>\n",
    "As such we obtain $S^\\top x = \\|(x^\\top L x)\\|_2 $  <br>\n",
    "As we have $x^\\top L x = \\frac{1}{2}\\sum W(i,j)(x[i]-x[j]) $ <br> \n",
    "Also we have $x^\\top L x = \\|(S^\\top x)\\|_2^2 $ <br>\n",
    "This gives $\\|(S^\\top x)\\|_2^2=\\frac{1}{2}\\sum W(i,j)(x[i]-x[j])$ <br>\n",
    "Then we obtain $|S^\\top x| = \\frac{1}{\\sqrt{2}} \\sqrt{W(i,j)}|x[i]-x[j]| $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Show that $\\lambda_k = \\| S^\\top u_k \\|_2^2$, where $\\| \\cdot \\|_2^2$ denotes the squared Euclidean norm (a.k.a. squared $L^2$ norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** <br>\n",
    "So we get $ L u_k = \\lambda_k u_k $ <br>\n",
    "As the eigenvectors are normalized, we can write it as $ u_k^\\top L u_k = \\lambda_k$ <br>\n",
    "Tt gives us $ u_k^\\top L u_k =u_k^\\top S^\\top S u_k = \\|S^\\top u_k \\|_2^2 = \\lambda_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the quantity $\\| S^\\top x \\|_2^2$ tell us about $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $S^\\top $ is similar to a gradient. As such, it gives us information about the smoothness of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What is the value of $u_0$, both for the combinatorial and normalized Laplacians?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $x^\\top L x = \\frac{1}{2}\\sum W(i,j)(x[i]-x[j])$ <br>\n",
    "$u_k^\\top L u_k = \\frac{1}{2}\\sum W(i,j)(u_k[i]-u_k[j])$ <br>\n",
    "$u_0^\\top L u_0 = \\frac{1}{2}\\sum W(i,j)(u_0[i]-u_0[j]) =0$ <br>\n",
    "As such, for each pair of vertices $(i,j)$ connected by an edge, we have $u_0(i) = u_0(j)$.  Thus, the signal value $u_0$ for all vertice must be a constant.  We conclude that the eigenspace of eigenvalue 0 has constant signal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Look at the spectrum of the Laplacian by plotting the eigenvalues.\n",
    "Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigenvalues, 'ro', markersize = 1)\n",
    "plt.title('Eigenspectrum')\n",
    "plt.ylabel('Eigenvalues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** There is a increasing trend in our eigenspectrum as expected since the first eigenvalue is always equal 0 and for the next eigenvalues, the value increases afterwards considering that our graph is fully connected. However, this increase in eigenvalues loses its initial acceleration very rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many connected components are there in your graph? Answer using the eigenvalues only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplicity of eigenvalue 0 gives connectedness of graph\n",
    "epsilon = 10**(-5)\n",
    "print(\"There are {} connected components.\".format(np.count_nonzero(eigenvalues<=epsilon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there an upper bound on the eigenvalues, i.e., what is the largest possible eigenvalue? Answer for both the combinatorial and normalized Laplacians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** For normalized Laplacians, the upperbound on the eigenvalues is equal to 2 due to the IFF bipartite graph. Additionally, for combinatorial Laplacians, due to Gershgorin circle theorem, the upperbound is bounded by the largest absolute row sum or column sum of combinatorial Laplacian matrix considering all the eigenvalues lie in the union of all Gershorin circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Laplacian eigenmaps\n",
    "\n",
    "*Laplacian eigenmaps* is a method to embed a graph $\\mathcal{G}$ in a $d$-dimensional Euclidean space.\n",
    "That is, it associates a vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$.\n",
    "The graph $\\mathcal{G}$ is thus embedded as $Z \\in \\mathbb{R}^{N \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What do we use Laplacian eigenmaps for? (Or more generally, graph embeddings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Laplacian eigenmaps, and graph embeddings in general, are used to reduce the dimensionality of data while remaining truthful to the original data. Usually, embeddings are used for visualization (reduction to 2 or 3 dimensions) but also for computation (reduce to a single dimension).\n",
    "\n",
    "This is done through the mapping of the graph to a vector space preserving the properties of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Embed your graph in $d=2$ dimensions with Laplacian eigenmaps.\n",
    "Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.\n",
    "\n",
    "**Recompute** the eigenvectors you need with a partial eigendecomposition method for sparse matrices.\n",
    "When $k \\ll N$ eigenvectors are needed, partial eigendecompositions are much more efficient than complete eigendecompositions.\n",
    "A partial eigendecomposition scales as $\\Omega(k |\\mathcal{E}|$), while a complete eigendecomposition costs $\\mathcal{O}(N^3)$ operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** We only need $k=3$ eigenvectors as we work in $d=2$. Thus, we use the *scipy.sparse.linalg.eigs* to compute the first 3 eigenvectors. Following our research in the original [Laplacian Eigenmaps Paper](http://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf) (cf bottom of p.6), we need to drop the eigenvector corresponding to the first eigenvalue ($\\lambda_1$) and pick the 2 next ones as our basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigs as sparse_eigs\n",
    "\n",
    "lap_sparse_eigenvals, lap_sparse_eigenvecs = sparse_eigs(laplacian, k=3, which='SM')\n",
    "lap_norm_sparse_eigenvecs = np.matmul(D_sqrt, lap_sparse_eigenvecs)\n",
    "lap_sparse_eigenvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keep the 2 dimensions we need by leaving out the eigenvector corresponding to the \"zero\" eigenvalue\n",
    "lap_sparse_eigenvecs = lap_sparse_eigenvecs[:,1:]\n",
    "lap_norm_sparse_eigenvecs = lap_norm_sparse_eigenvecs[:,1:]\n",
    "lap_sparse_eigenvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the nodes embedded in 2D. Comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lap_sparse_eigenvecs[:,0].real, lap_sparse_eigenvecs[:,1].real, 'ro', markersize=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lap_norm_sparse_eigenvecs[:,0].real, lap_norm_sparse_eigenvecs[:,1].real, 'ro', markersize=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We prefer keeping the normalized values. Indeed, we can see more clearly the differerences in the data (upper right corner and line starting in the bottom right corner) as some clusters form. This is despite the fact that the values are closer to each other (_x_ ranges from $-0.3$ to $0$ instead of $-0.3$ to $0$, while _y_ ranges from $-0.025$ to $0.05$ instead of ranging up to $0.175$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the embedding $Z \\in \\mathbb{R}^{N \\times d}$ preserve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The embedding $Z$ preserves similarity. As explained in the Slides, \"we want similar points to be embedded to each other\". Given that we work with graphs, this similarity is nothing but the distance in the projected space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Spectral clustering\n",
    "\n",
    "*Spectral clustering* is a method to partition a graph into distinct clusters.\n",
    "The method associates a feature vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$, then runs [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering) in the embedding space $\\mathbb{R}^d$ to assign each node $v_i \\in \\mathcal{V}$ to a cluster $c_j \\in \\mathcal{C}$, where $k = |\\mathcal{C}|$ is the number of desired clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Choose $k$ and $d$. How did you get to those numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** In the eigenspectrum plotted in Question $6$, we see a clear gap between the first 3 eigenvalues and all of the others. Thus, we choose $k=3$ following the instructions on the Slides. (\"If data has k clear clusters, there will be a gap in the Laplacian spectrum after the k-th eigenvalue. Use to choose k.\").\n",
    "\n",
    "On the other hand, we choose $d=2$ in order to have a better visualization. Note that we have tried working with $d=3$, but it does not give much more additional information (especially in the case of the normalized eigenvectors, given that all the points are on the same plane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "1. Embed your graph in $\\mathbb{R}^d$ as $Z \\in \\mathbb{R}^{N \\times d}$.\n",
    "   Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.\n",
    "1. If you want $k=2$ clusters, partition with the Fiedler vector. For $k > 2$ clusters, run $k$-means on $Z$. Don't implement $k$-means, use the `KMeans` class imported from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_eigenvals, sparse_eigenvecs = sparse_eigs(laplacian, k=3, which='SM')\n",
    "kmeans = KMeans(n_clusters=3).fit_predict(sparse_eigenvecs.real)\n",
    "plt.scatter(sparse_eigenvecs[:,1].real, sparse_eigenvecs[:,2].real, c=kmeans, s=20, cmap='cool')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(sparse_eigenvecs[:,1].real, sparse_eigenvecs[:,2].real, sparse_eigenvecs[:,0].real, c=kmeans, cmap='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sparse_eigenvecs = np.matmul(D_sqrt, sparse_eigenvecs)\n",
    "norm_kmeans = KMeans(n_clusters=3).fit_predict(norm_sparse_eigenvecs.real)\n",
    "plt.scatter(norm_sparse_eigenvecs[:,1].real, norm_sparse_eigenvecs[:,2].real, c=norm_kmeans, s=20, cmap='cool')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(norm_sparse_eigenvecs[:,1].real, norm_sparse_eigenvecs[:,2].real, norm_sparse_eigenvecs[:,0].real, c=norm_kmeans, cmap='cool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** As told in the lecture, \"normalization seeks to impose balanced clusters.\" This fact can be seen on the second 3-D plot. We choose using the eigenvectors which have not been re-normalized as the clusters are more straight-forward to understand, but also farthest from each other. Moreover, the K-means algorithm applied to the re-normalized eigenvectors is prone to initialization error (the assignment of points is different with each run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "Use the computed cluster assignment to reorder the adjacency matrix $A$.\n",
    "What do you expect? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_idxs2, xi = zip(*sorted(zip(norm_kmeans,range(len(norm_kmeans)))))\n",
    "y_idxs2, yi = zip(*sorted(zip(norm_kmeans,range(len(norm_kmeans)))))\n",
    "ordered_adj = adjacency[xi,:][:,yi]\n",
    "ordered_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Given the nature of clusters, we expect to see cliques appearing in the matrix. However, they will not be perfect as our network is comprised of a single connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ordered_adj != 0, xticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** This is indeed what we notice when looking at the heatmap of the ordered adjacency. We can see clearly the big rectangle representing the biggest cluster. However, we also notice that the rest of the matrix is not as tidy. Indeed, as expected, we notice some links between the biggest cluster and the others. But we cannot notice the smaller clusters, maybe because they are too small and cannot be considered as relevant ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ordered_adj[:500,:500] != 0, xticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When zooming, we see that the only reason it didn't seem to have any ordering is because of the size of the cluster (around $350$ nodes only). We also notice that the $2^{nd}$ cluster is very small and very compact, as it is only comprised of around 15 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "If you have ground truth clusters for your dataset, compare the cluster assignment from spectral clustering to the ground truth.\n",
    "A simple quantitative measure is to compute the percentage of nodes that have been correctly categorized.\n",
    "If you don't have a ground truth, qualitatively assess the quality of the clustering.\n",
    "\n",
    "Ground truth clusters are the \"real clusters\".\n",
    "For example, the genre of musical tracks in FMA, the category of Wikipedia articles, the spammer status of individuals, etc.\n",
    "Look for the `labels` in the [dataset descriptions](https://github.com/mdeff/ntds_2018/tree/master/projects/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We do not have ground truth assignments, thus we look at the quality of clusters following the previous rearrangement of the adjacency matrix (cf Question 12). We can clearly see the 3 different clusters, showing the weak communities of our network. In the 2 following graphs, we see that the left side (respectively right side) is more densely populated, meaning there are more links with itself. However, note that this trend not very distinguishable in the 1st graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(ordered_adj[:354] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ordered_adj[363:] != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can question the existence of the $2^{nd}$ community, which is only constituted by around $15$ actors. When looking more closely, we see that this community is very strongly connected, while it is only very loosely connected to other nodes (which are far away)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(ordered_adj[354:363, 354:363] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(ordered_adj[354:363] != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "Plot the cluster assignment (one color per cluster) on the 2D embedding you computed above with Laplacian eigenmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lap_norm_sparse_eigenvecs[:,0].real, lap_norm_sparse_eigenvecs[:,1].real, c=norm_kmeans, cmap='cool', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "\n",
    "Why did we use the eigenvectors of the graph Laplacian as features? Could we use other features for clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The idea behind using the eigenvectors of the graph Laplacian as features is similar as using Laplacian eigenmaps; it is the best way to reduce the dimensionality of the data in order to cluster it. \n",
    "\n",
    "More specifically, the eigenvectors splits the nodes into $\\textit{k}$ clusters \"blindly\", meaning it uses $\\textit{k}$ partition signals which depend on the distance between the nodes, which is represented by the feature vector comprised of the $\\textit{k}$ first eigenvectors of the graph Laplacian.\n",
    "\n",
    "The only other feature we could have used from our original data is \"gender\". Indeed, it is the only feature that allows us to cluster data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
