{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "from pyunlocbox import functions, solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinan Bursa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits = pd.read_csv('data/tmdb_5000_credits.csv')\n",
    "credits = credits[credits.cast != '[]']\n",
    "\n",
    "\n",
    "movies = pd.read_csv('data/tmdb_5000_movies.csv')\n",
    "movies.drop(['homepage', 'keywords','original_language','overview','release_date','spoken_languages', \\\n",
    "             'status','title','tagline','vote_count'\\\n",
    "            ], \\\n",
    "            axis=1, \\\n",
    "            inplace=True \\\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits.drop(['title', 'crew'], axis=1, inplace=True)\n",
    "credits['cast_id'] = credits['cast'].apply(lambda row: list(set(pd.read_json(row)['id'])))\n",
    "#credits['cast_name'] = credits['cast'].apply(lambda row: list(set(pd.read_json(row)['name'])))\n",
    "#credits['gender'] = credits['cast'].apply(lambda row: list(set(pd.read_json(row)['gender'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = pd.DataFrame()\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for idx, film in credits.iterrows():\n",
    "    cast_df = pd.DataFrame(eval(credits['cast'][idx]))\n",
    "    cast_df['credits'] = idx\n",
    "    cast_df = cast_df.drop(['character','order', 'credit_id', 'cast_id'],axis = 1)  \n",
    "    \n",
    "    frames = [new_df, cast_df]\n",
    "    new_df = pd.concat(frames, join = 'outer', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_old = credits['cast_id'].apply(pd.Series).stack().value_counts()\n",
    "discount_old = list(discount_old[discount_old > 4].index.astype(int))\n",
    "#discount_old[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = new_df['credits'].groupby([new_df.gender, new_df.id, new_df.name]).apply(list).reset_index()\n",
    "nodes_df = nodes_df[nodes_df['gender'].isin(['1','2'])]\n",
    "discount_1 = nodes_df['id'].tolist()\n",
    "discount = [x for x in discount_old if x in discount_1]\n",
    "#nodes_df = nodes_df[nodes_df.id.isin(discount)]\n",
    "#nodes_df.drop(columns=['credits'], inplace=True)\n",
    "#nodes_df = nodes_df[nodes_df['gender'].isin(['1','2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old Values of the Discount')\n",
    "print(discount_old[:10])\n",
    "print(len(discount_old))\n",
    "print('New Values of the Discount')\n",
    "print(discount[:10])\n",
    "print(len(discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits['cast_id'] = credits['cast_id'].apply(lambda x: [y for y in x if y in discount])\n",
    "credits['edges'] = credits['cast_id'].apply(lambda x: list(itertools.combinations(x, 2)))\n",
    "edges = list(credits['edges'].apply(pd.Series).stack())\n",
    "edges[0:5]\n",
    "\n",
    "edges_df = pd.DataFrame(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges) #Our edges changed as we removed the gender 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_movies = set()\n",
    "\n",
    "for idx, movie in credits.iterrows():\n",
    "    if len(movie['edges']) == 0:\n",
    "        discarded_movies.add(movie['movie_id'])\n",
    "\n",
    "print(len(discarded_movies)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "credits = credits[~credits['movie_id'].isin(discarded_movies)]\n",
    "credits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['profit'] = movies['revenue']-movies['budget']\n",
    "movies_credits = movies.merge(credits, left_on='id', right_on='movie_id', how='inner').drop(columns=['movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_credits = movies_credits[movies_credits.genres != '[]']\n",
    "movies_credits['genre_id'] = movies_credits['genres'].apply(lambda row: list(set(pd.read_json(row)['id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_df = pd.DataFrame(movies_credits.cast_id.tolist(), index=movies_credits.profit).stack().reset_index(name='cast_id')[['cast_id','profit']]\n",
    "profit_df['cast_id'] = profit_df.cast_id.astype(int)\n",
    "profit_df = profit_df.groupby('cast_id', as_index=False).mean()\n",
    "profit_df.set_index('cast_id', inplace=True)\n",
    "profit_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = pd.DataFrame(movies_credits.cast_id.tolist(), index=movies_credits.vote_average).stack().reset_index(name='cast_id')[['cast_id','vote_average']]\n",
    "ranking_df['cast_id'] = ranking_df.cast_id.astype(int)\n",
    "ranking_df = ranking_df.groupby('cast_id', as_index=False).mean()\n",
    "ranking_df.set_index('cast_id', inplace=True)\n",
    "ranking_df.head()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "genre = movies_credits[['cast_id', 'genre_id']]\n",
    "genre.loc[:, 'genre_id_disc'] = genre['genre_id'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = pd.DataFrame(genre.cast_id.tolist(), index=genre.genre_id_disc).stack().reset_index(name='cast_id')[['cast_id','genre_id_disc']]\n",
    "most_freq_genre = genre_df.groupby(['cast_id']).agg(lambda x:x.value_counts().index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = ranking_df.merge(most_freq_genre, on='cast_id', how='inner')\n",
    "actors = actors.merge(profit_df, on='cast_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = actors.reset_index()\n",
    "actors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I MOVED IT UP SO NO NEED. I KEPT IT JUST TO BE SAFE\n",
    "#frames = pd.DataFrame()\n",
    "#new_df = pd.DataFrame()\n",
    "\n",
    "#for idx, film in credits.iterrows():\n",
    "#    cast_df = pd.DataFrame(eval(credits['cast'][idx]))\n",
    "#    cast_df['credits'] = idx\n",
    "#    cast_df = cast_df.drop(['character','order', 'credit_id', 'cast_id'],axis = 1)  \n",
    "    \n",
    "#    frames = [new_df, cast_df]\n",
    "#    new_df = pd.concat(frames, join = 'outer', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes_df = new_df['credits'].groupby([new_df.gender, new_df.id, new_df.name]).apply(list).reset_index()\n",
    "nodes_df = nodes_df[nodes_df.id.isin(discount)]\n",
    "nodes_df.drop(columns=['credits'], inplace=True)\n",
    "#nodes_df = nodes_df[nodes_df['gender'].isin(['1','2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = actors.merge(nodes_df, left_on = 'cast_id', right_on='id', how='inner').drop(columns=['cast_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = actors[['name', 'id', 'gender', 'genre_id_disc', 'vote_average', 'profit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors[actors['name']=='Leonardo DiCaprio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors.sort_values(by='profit', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#features = nodes_df.set_index('id').drop('name', axis=1)\n",
    "#features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_df = pd.DataFrame(discount)\n",
    "features = discount_df.merge(actors, left_on = 0, right_on='id', how='inner').drop(columns=[0])\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bechdel Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bechdel.pkl', 'rb') as pkl_file: \n",
    "    movies_bechdel = pickle.load(pkl_file)\n",
    "    \n",
    "movies_bechdel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the Adjacency again\n",
    "Cause we took out some genders and our size went from 3766 to 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = pd.DataFrame(np.zeros(shape=(len(discount),len(discount))), columns=discount, index=discount)\n",
    "for e1, e2 in edges:\n",
    "    if e1 in discount and e2 in discount:\n",
    "        adj.at[e1, e2] += 1\n",
    "        adj.at[e2, e1] += 1\n",
    "    else:\n",
    "        edges.remove((e1,e2))\n",
    "\n",
    "adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = adj.values\n",
    "adj_max = adjacency.max()\n",
    "adjacency = np.vectorize(lambda x: x/adj_max)(adjacency)\n",
    "\n",
    "adjacency = pd.DataFrame(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF WE NEED NON WEIGHTED ADJACENCY\n",
    "adjacency_non_weighted = np.copy(adjacency)\n",
    "adjacency_non_weighted[adjacency_non_weighted > 0] = 1\n",
    "adjacency_non_weighted = np.asmatrix(adjacency_non_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.from_numpy_array(adjacency_non_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_props = features.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in node_props:\n",
    "    nx.set_node_attributes(graph, node_props[key], key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.node[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(graph, 'actorsGephiFile.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1  (from solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjacency = pd.read_csv('data/adjacency.csv')\n",
    "n_nodes = len(adjacency)\n",
    "n_nodes\n",
    "#Dropping useless column from adjacency dataframe\n",
    "#adjacency.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "#adjacency = adjacency.values\n",
    "#np.set_printoptions(suppress = True)\n",
    "#n_nodes = len(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = pd.read_csv('data/features.csv') #--> NO NEED CAUSE WE HAVE IT AT THE TOP\n",
    "labels = features.reset_index()\n",
    "id_to_idx = dict(zip(labels['id'], labels.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edges = pd.read_csv('data/edges.csv')\n",
    "#edges.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "edges = pd.DataFrame(edges)\n",
    "edges.head()\n",
    "edges[0] = edges[0].map(id_to_idx)\n",
    "edges[1] = edges[1].map(id_to_idx)\n",
    "#edges['0'] = edges['0'].map(id_to_idx)\n",
    "#edges['1'] = edges['1'].map(id_to_idx)\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_edges = len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency= pd.DataFrame(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomputing the Laplacian\n",
    "D = np.diag(np.sum(adjacency, 1)) # Degree matrix\n",
    "D_norm = np.diag(np.sum(adjacency, 1)**(-1/2)) \n",
    "laplacian_combinatorial =  D - adjacency\n",
    "laplacian_normalized =  D_norm @ laplacian_combinatorial @ D_norm\n",
    "\n",
    "laplacian = laplacian_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE DONT USE THE GRADIENT SO I THINK WE NEED IT\n",
    "# Computing the gradient (incident matrix S)\n",
    "S = np.zeros((n_nodes, n_edges))\n",
    "edge_idx = 0\n",
    "for i in range(n_nodes):\n",
    "    for k in range(i):\n",
    "        if adjacency.iloc[i,k] == 1.0:\n",
    "            S[i,edge_idx] = 1\n",
    "            S[k,edge_idx] = 1\n",
    "            edge_idx += 1\n",
    "            \n",
    "assert np.allclose(S @ S.T, laplacian_combinatorial) \n",
    "# THIS ABOVE DOESNT WORK MOST LIKELY BECAUSE THEY USED NON WEIGHTED ADJACENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Fourier basis vectors and the Laplacian eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, U = np.linalg.eigh(laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Eigenvalues are already sorted in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "(markerline, stemlines, baseline)=plt.stem(np.arange(len(e)),e);\n",
    "plt.setp(baseline, visible=False)\n",
    "plt.ylim((0,1.2))\n",
    "plt.xlim((-1,98))\n",
    "plt.ylabel('$\\lambda$');\n",
    "plt.xlabel('Eigenvalue index');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first 3 and the last Fourier basis vectors as signals on your graph. Clearly indicate which plot belongs to which basis vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_norm = np.diag(np.sum(adjacency, 1)**(-1/2)) \n",
    "network_emb = D_norm @ U[:,[1,3]]\n",
    "emb_x = network_emb[:,0]\n",
    "emb_y = network_emb[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the first four and two last Fourier basis vectors instead to see some more variation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15,17))\n",
    "\n",
    "fourier_bases = [0,1,2,3,len(adjacency)-2,len(adjacency)-1]\n",
    "\n",
    "vmax = max(-U[:,fourier_bases].min(), U[:,fourier_bases].max())\n",
    "vmin = -vmax\n",
    "\n",
    "for ax_idx, fourier_basis in enumerate(fourier_bases):\n",
    "    ax_x, ax_y = ax_idx//2, ax_idx%2\n",
    "    im = ax[ax_x, ax_y].scatter(emb_x, emb_y, c=U[:,fourier_basis], cmap='bwr', s=70, \n",
    "                                edgecolors='black', vmin=vmin, vmax=vmax)\n",
    "    ax[ax_x, ax_y].set_title('Signal = Fourier basis {}'.format(fourier_basis))\n",
    "    ax[ax_x, ax_y].set_xlabel('Generalized eigenvector embedding $U_1$')\n",
    "    ax[ax_x, ax_y].set_ylabel('Generalized eigenvector embedding $U_3$')\n",
    "    \n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.925, 0.15, 0.025, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to our own embedding using two chosen Eigenvectors, we would also like to display all graphs using NetworkX's Force-directed layout. This finds the party separation between the two main clusters and arranges them nicely (with one exception).\n",
    "\n",
    "Also, in this representation we can plot all the edges without distracting too much from the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.from_numpy_matrix(adjacency.as_matrix())\n",
    "coords = nx.spring_layout(graph) # Force-directed layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15,15))\n",
    "\n",
    "fourier_bases = [0,1,2,3,96,97]\n",
    "\n",
    "vmax = max(-U[:,fourier_bases].min(), U[:,fourier_bases].max())\n",
    "vmin = -vmax\n",
    "\n",
    "for ax_idx, fourier_basis in enumerate(fourier_bases):\n",
    "    ax_x, ax_y = ax_idx//2, ax_idx%2\n",
    "    im = nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=U[:,fourier_basis], \n",
    "                                cmap='bwr', edgecolors='black', ax=ax[ax_x, ax_y], vmin=vmin, vmax=vmax)\n",
    "    nx.draw_networkx_edges(graph, coords, alpha=0.2, ax=ax[ax_x, ax_y])\n",
    "    ax[ax_x, ax_y].set_title('Signal = Fourier basis {}'.format(fourier_basis))\n",
    "    \n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.925, 0.15, 0.025, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you observe in terms of local variations when comparing the basis vectors corresponding to the smallest eigenvalues to those corresponding to the largest eigenvalue? How would this justify the interpretation of the eigenvalues as \"graph frequencies\"?\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "For the signals corresponding to the smallest eigenvalues, we can observe them having very low frequencies over the network while still having very large weights. The basis vectors of larger eigenvalues contain mostly lower amplitudes and slightly higher frequencies. The one outlier node has a large influence over some of the Fourier bases, like basis 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Fourier transform (GFT) and Inverse Graph Fourier transform (iGFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GFT(x):\n",
    "    return U.T @ x #The matmul function implements the semantics of the @ operator introduced in Python 3.5\n",
    "\n",
    "def iGFT(x):\n",
    "    return U @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your feature/label vector as a signal on your graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coplot_network_signal(signal, title='Signal = ...'):\n",
    "    '''\n",
    "    Plots a signal on a graph using both a Laplacian embedding and the NetworkX force-directed layout.\n",
    "    \n",
    "    Args:\n",
    "        signal: The signal of each node to plot on the graph\n",
    "        title: Plot title\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16,7))\n",
    "        \n",
    "    vmax = max(-np.nanmin(signal), np.nanmax(signal))\n",
    "    vmin = -vmax\n",
    "\n",
    "    im = ax[0].scatter(emb_x, emb_y, c=signal, cmap='bwr', s=70, edgecolors='black', \n",
    "                       vmin=vmin, vmax=vmax)\n",
    "    ax[0].set_title('Laplacian Embedding')\n",
    "    ax[0].set_xlabel('Generalized eigenvector embedding $U_1$')\n",
    "    ax[0].set_ylabel('Generalized eigenvector embedding $U_3$')\n",
    "\n",
    "    nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=signal, cmap='bwr', \n",
    "                           edgecolors='black', ax=ax[1], vmin=vmin, vmax=vmax)\n",
    "    nx.draw_networkx_edges(graph, coords, alpha=0.2, ax=ax[1])\n",
    "    ax[1].set_title('NetworkX Force-directed layout')\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cbar_ax = fig.add_axes([0.925, 0.15, 0.025, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = features['gender']\n",
    "labels_genre = features['genre_id_disc']\n",
    "labels_vote = features['vote_average']\n",
    "labels_profit = features['profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coplot_network_signal(labels, title='Signal = Ground truth labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the absolute values of the GFT of your feature/label signal as a function of the graph eigenvalues. Make sure to add a marker indicating the position of each graph eigenvalue, and remember to properly name the axes.\n",
    "\n",
    "We plot the signal with respect to the (normalized) Eigenvalue frequency as well as the respective index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "(markerline, stemlines, baseline)=plt.stem(np.arange(len(e)),abs(GFT(labels)));\n",
    "plt.setp(baseline, visible=False)\n",
    "plt.ylim((0,10))\n",
    "plt.xlim((-1,98))\n",
    "plt.title('Graph Fourier Transform of the real label signal')\n",
    "plt.ylabel('$|U^T x|$');\n",
    "plt.xlabel('Eigenvalue index');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "(markerline, stemlines, baseline)=plt.stem(e,abs(GFT(labels)));\n",
    "plt.setp(baseline, visible=False)\n",
    "plt.ylim((0,10))\n",
    "plt.title('Graph Fourier Transform of the real label signal')\n",
    "plt.ylabel('$|U^T x|$');\n",
    "plt.xlabel('Eigenvalue frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_kernel(e, t):\n",
    "    return np.exp(-t * e)\n",
    "\n",
    "def inverse_kernel(e, t):\n",
    "    return 1/(1 + t*e)\n",
    "\n",
    "def rectangle_kernel(e, l_min, l_max):\n",
    "    return ((e >= l_min) & (e <= l_max)).astype(float)\n",
    "\n",
    "def graph_filter(x, kernel, **kwargs):\n",
    "    return iGFT(kernel(e, **kwargs) * GFT(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all three filter kernels in the spectral domain. Remember to properly name the axes and title the plots. Choose filter parameters that best approximate the behavior of the GFT of your feature/label signal (as seen in Question 4).\n",
    "\n",
    "We plot the signal with respect to the (normalized) Eigenvalue frequency as well as the respective index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(20,16))\n",
    "\n",
    "kernels = {\n",
    "    'Heat kernel': heat_kernel(e,5),\n",
    "    'Inverse kernel': inverse_kernel(e,10),\n",
    "    'Rectangular kernel': rectangle_kernel(e,0.1,0.8)\n",
    "}\n",
    "\n",
    "for idx, (kernel_name, kernel) in enumerate(kernels.items()):    \n",
    "    (markerline, stemlines, baseline)=ax[idx].stem(np.arange(len(e)), kernel);\n",
    "    plt.setp(baseline, visible=False)\n",
    "    ax[idx].set_ylim((0,1.1))\n",
    "    ax[idx].set_xlim((-1,98))\n",
    "    ax[idx].set_title(kernel_name)\n",
    "    ax[idx].set_ylabel('Kernel strength');\n",
    "    ax[idx].set_xlabel('Eigenvalue index');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(20,16))\n",
    "\n",
    "kernels = {\n",
    "    'Heat kernel': heat_kernel(e,5),\n",
    "    'Inverse kernel': inverse_kernel(e,10),\n",
    "    'Rectangular kernel': rectangle_kernel(e,0.1,0.8)\n",
    "}\n",
    "\n",
    "for idx, (kernel_name, kernel) in enumerate(kernels.items()):    \n",
    "    (markerline, stemlines, baseline)=ax[idx].stem(e, kernel);\n",
    "    plt.setp(baseline, visible=False)\n",
    "    ax[idx].set_ylim((0,1.1))\n",
    "    ax[idx].set_title(kernel_name)\n",
    "    ax[idx].set_ylabel('Kernel strength');\n",
    "    ax[idx].set_xlabel('Eigenvalue frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the filter parameters such that the second peak gets preserved well, while larger frequencies are attenuated. With the rectangle kernel, we can very easily select the desired frequencies, while with the other two we have to find some equilibrium between the strength of the desired signal and the rest. In case of the heat kernel, we could nicely isolate the second peak while not having to worry about the first one, as it models a constant signal over the network. In the inverse kernel, we still have a tail with a lot of weight.\n",
    "\n",
    "When filtering, we multiply this filter kernel by the Fourier spectrum of a signal, whereby the low frequencies are kept and the larger frequencies are discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two Dirac impulses arbitrarily placed on your graph. Plot their filtered versions by the three filter kernels implemented in Question 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filtered_diracs(dirac_dict, kernel_name, kernel, **kwargs):\n",
    "    ''' \n",
    "    Plots the filtered signal of two randomly chosen dirac spikes on a graph. \n",
    "    Plots using both Laplacian embedding and NetworkX Force-directed layout.\n",
    "    \n",
    "    Args:\n",
    "        dirac_dict: Dictionary specifying index and value of diracs. Eg: {30: 1, 60: -1}\n",
    "        kernel_name: Used in the title to indicate used kernel type\n",
    "        kernel: Kernel function to apply to diracs\n",
    "        kwargs: Additional arguments for specific kernel\n",
    "    '''\n",
    "    diracs = np.zeros(n_nodes)\n",
    "    dirac_idxs = list(dirac_dict.keys())\n",
    "    for idx, dirac_value in dirac_dict.items():\n",
    "        diracs[idx] = dirac_value\n",
    "\n",
    "    filtered = graph_filter(diracs, kernel, **kwargs)\n",
    "\n",
    "    vmax = max(-filtered.min(), filtered.max())\n",
    "    vmin = -vmax\n",
    "\n",
    "    # Plot using Laplacian embedding\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16,7))\n",
    "    # Plotting all nodes\n",
    "    ax[0].scatter(emb_x, emb_y, c=filtered, cmap='PiYG', s=70, \n",
    "                  edgecolors='black', vmin=vmin, vmax=vmax)\n",
    "    # Plotting the dirac locations\n",
    "    im = ax[0].scatter(emb_x[dirac_idxs], emb_y[dirac_idxs], c=diracs[dirac_idxs], \n",
    "                       cmap='PiYG', s=300, edgecolors='black', vmin=vmin, vmax=vmax, marker='x')\n",
    "    ax[0].set_title('Laplacian Embedding')\n",
    "    ax[0].set_xlabel('Generalized eigenvector embedding $U_1$')\n",
    "    ax[0].set_ylabel('Generalized eigenvector embedding $U_3$')\n",
    "\n",
    "    # Plot using NetworkX\n",
    "    nx.draw_networkx_nodes(graph, coords, node_size=60, node_color=filtered, \n",
    "                           cmap='PiYG', edgecolors='black', ax=ax[1], vmin=vmin, vmax=vmax)\n",
    "    nx.draw_networkx_edges(graph, coords, alpha=0.2, ax=ax[1])\n",
    "    # Plotting the dirac locations\n",
    "    d1_coords = coords[dirac_idxs[0]]\n",
    "    d2_coords = coords[dirac_idxs[1]]\n",
    "    im = ax[1].scatter(d1_coords[0], d1_coords[1], c=diracs[dirac_idxs[0]], \n",
    "                       cmap='PiYG', s=300, edgecolors='black', vmin=vmin, vmax=vmax, marker='x')\n",
    "    im = ax[1].scatter(d2_coords[0], d2_coords[1], c=diracs[dirac_idxs[1]], \n",
    "                       cmap='PiYG', s=300, edgecolors='black', vmin=vmin, vmax=vmax, marker='x')\n",
    "    ax[1].set_title('NetworkX Force-directed layout')\n",
    "\n",
    "    fig.suptitle('Signal = Two diracs filtered using {}'.format(kernel_name), fontsize=16)\n",
    "\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cbar_ax = fig.add_axes([0.925, 0.15, 0.025, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plots, as our graph consists of two main clusters, we will plot the following possible combinations of placing two dirac spikes:\n",
    "\n",
    "8.1: Diracs of opposite signs in opposite clusters\n",
    "8.2: Diracs of opposite signs in same cluster\n",
    "8.3: Diracs of equal signs in opposite clusters\n",
    "8.4: Diracs of equal signs in same cluster\n",
    "For reproducibility reasons we will hereby not initialize the nodes randomly, but choose them arbitrarily ourself. We chose nodes 15, 30 and 60 for that purpose.\n",
    "\n",
    "We plot the bellow graphs using a different color palette, as not to conflict the colours with any party associations in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Diracs of opposite signs in opposite clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: -1, 60: 1}, 'Heat Kernel', heat_kernel, t=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: -1, 60: 1}, 'Inverse Kernel', inverse_kernel, t=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: -1, 60: 1}, 'Rectangle Kernel', rectangle_kernel, l_min=0.1, l_max=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Diracs of opposite signs in same clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: -1, 15: 1}, 'Heat Kernel', heat_kernel, t=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: -1, 15: 1}, 'Inverse Kernel', inverse_kernel, t=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: -1, 15: 1}, 'Rectangle Kernel', rectangle_kernel, l_min=0.1, l_max=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Diracs of equal signs in opposite clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: 1, 60: 1}, 'Heat Kernel', heat_kernel, t=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: 1, 60: 1}, 'Inverse Kernel', inverse_kernel, t=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: 1, 60: 1}, 'Rectangle Kernel', rectangle_kernel, l_min=0.1, l_max=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Diracs of equal signs in same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: 1, 15: 1}, 'Heat Kernel', heat_kernel, t=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: 1, 15: 1}, 'Inverse Kernel', inverse_kernel, t=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_diracs({30: 1, 15: 1}, 'Rectangle Kernel', rectangle_kernel, l_min=0.1, l_max=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, set the noise variance $\\sigma^2$ by making sure that the signal-to-noise ratio $SNR = \\frac{\\operatorname{Var}(\\text{labels})}{\\sigma^2}$ is about  $1.5$.\n",
    "\n",
    "_Note:_ Actually, you might want to play with the noise variance here and set it to different values and see how the denoising filters behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_variance = labels.std()**2 / 1.5\n",
    "noisy_measurements = labels + noise_variance * np.random.randn(n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, denoise the noisy measurements by passing them through the filters that you implemented in Question 6. Choose the filter parameters based on the behavior of the GFT of your original label signal (this is the prior knowledge that you input to the problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot, on your graph, the original label signal, the noisy measurements, and the three denoised version obtained above. Report on each plot the value of the corresponding relative error \n",
    "$$\n",
    "\\text{rel-err} = \\frac{\\|\\text{labels} - z \\|_2}{\\|\\text{labels}\\|_2},\n",
    "$$\n",
    "where $z$ is the plotted signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_err(labels, z):\n",
    "    ''' \n",
    "    Calculates the relative error between the true labels and an estimate z\n",
    "    \n",
    "    Args:\n",
    "        labels: Ground truth signal\n",
    "        z: Estimated signal\n",
    "    '''\n",
    "    return np.linalg.norm(labels - z, 2) / np.linalg.norm(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coplot_network_signal(labels, title='Signal = Ground truth labels')\n",
    "print('Relative Error: {:.2f}'.format(rel_err(labels, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coplot_network_signal(noisy_measurements, title='Signal = Noisy measurements')\n",
    "print('Relative Error: {:.2f}'.format(rel_err(labels, noisy_measurements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_heat_denoised = graph_filter(noisy_measurements, heat_kernel, t=5)\n",
    "coplot_network_signal(z_heat_denoised, title='Signal = Heat denoised measurements')\n",
    "print('Relative Error: {:.2f}'.format(rel_err(labels, z_heat_denoised)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_inv_denoised = graph_filter(noisy_measurements, inverse_kernel, t=5)\n",
    "coplot_network_signal(z_inv_denoised, title='Signal = Inverse denoised measurements')\n",
    "print('Relative Error: {:.2f}'.format(rel_err(labels, z_inv_denoised)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_rect_denoised = graph_filter(noisy_measurements, rectangle_kernel, l_min=0.1, l_max=0.8)\n",
    "coplot_network_signal(z_rect_denoised, title='Signal = Rectangle denoised measurements')\n",
    "print('Relative Error: {:.2f}'.format(rel_err(labels, z_rect_denoised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, overlay on the same plot the GFT of all five signals above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = {\n",
    "    'Ground truth': labels,\n",
    "    'Noisy measurements': noisy_measurements,\n",
    "    'Heat filtered': z_heat_denoised,\n",
    "    'Inverse filtered': z_inv_denoised,\n",
    "    'Rectangle filtered': z_rect_denoised\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for signal_name, signal in signals.items():\n",
    "    plt.plot(signal, label=signal_name)\n",
    "    \n",
    "plt.xlabel('Node index')\n",
    "plt.ylabel('Signal strength')\n",
    "plt.title('Signals on sorted nodes')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
